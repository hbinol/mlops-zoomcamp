{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b194c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.0 MB 8.7 MB/s eta 0:00:01     |█████████████████▏              | 15.6 MB 8.7 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting gunicorn<24\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting graphene<4\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 81.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.4.32)\n",
      "Requirement already satisfied: Flask<4 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.1.2)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (2.11.3)\n",
      "Requirement already satisfied: matplotlib<4 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (3.5.1)\n",
      "Requirement already satisfied: pandas<3 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.4.2)\n",
      "Collecting pyarrow<20,>=4.0.0\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.1 MB 79 kB/s s eta 0:00:01     |██████████████████████▉         | 30.0 MB 78.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<3 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.21.5)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 89.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mlflow-skinny==2.22.0\n",
      "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 61.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<2 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (1.7.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow) (3.3.4)\n",
      "Collecting docker<8,>=4.0.0\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 91.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<7,>=3.12.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (3.19.1)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 5.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 94.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=5.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (2.0.0)\n",
      "Requirement already satisfied: packaging<25 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (21.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (6.0)\n",
      "Collecting uvicorn<1\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting databricks-sdk<1,>=0.20.0\n",
      "  Downloading databricks_sdk-0.54.0-py3-none-any.whl (720 kB)\n",
      "\u001b[K     |████████████████████████████████| 720 kB 78.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-sdk<3,>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 88.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.17.3 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (2.27.1)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from mlflow-skinny==2.22.0->mlflow) (4.1.1)\n",
      "Collecting pydantic<3,>=1.10.8\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[K     |████████████████████████████████| 444 kB 63.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-api<3,>=1.9.0\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi<1\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tomli in /home/codespace/anaconda3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.2)\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.0.0\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.17.3\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth~=2.0\n",
      "  Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 83.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\n",
      "Collecting starlette<0.47.0,>=0.40.0\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 886 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /home/codespace/anaconda3/lib/python3.9/site-packages (from Flask<4->mlflow) (2.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/codespace/anaconda3/lib/python3.9/site-packages (from Flask<4->mlflow) (2.0.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/codespace/anaconda3/lib/python3.9/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from graphene<4->mlflow) (2.8.2)\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[K     |████████████████████████████████| 203 kB 90.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/codespace/anaconda3/lib/python3.9/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/codespace/anaconda3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/anaconda3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/codespace/anaconda3/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.12.1)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 89.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from pandas<3->mlflow) (2021.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/codespace/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.8)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 63.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/anaconda3/lib/python3.9/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (2.0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/codespace/anaconda3/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (1.1.1)\n",
      "Collecting anyio<5,>=3.6.2\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 19.1 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (1.2.0)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zipp, typing-extensions, importlib-metadata, exceptiongroup, deprecated, typing-inspection, smmap, pydantic-core, opentelemetry-api, cachetools, anyio, annotated-types, starlette, requests, pydantic, opentelemetry-semantic-conventions, h11, graphql-core, google-auth, gitdb, uvicorn, sqlparse, opentelemetry-sdk, Mako, graphql-relay, gitpython, fastapi, databricks-sdk, pyarrow, mlflow-skinny, gunicorn, graphene, docker, alembic, mlflow\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.7.0\n",
      "    Uninstalling zipp-3.7.0:\n",
      "      Successfully uninstalled zipp-3.7.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "    Uninstalling pyarrow-20.0.0:\n",
      "      Successfully uninstalled pyarrow-20.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "jupyter-server 1.13.5 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.40.2 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.40.2 which is incompatible.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.40.2 which is incompatible.\u001b[0m\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.1 annotated-types-0.7.0 anyio-4.9.0 cachetools-5.5.2 databricks-sdk-0.54.0 deprecated-1.2.18 docker-7.1.0 exceptiongroup-1.3.0 fastapi-0.115.12 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.40.2 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.16.0 importlib-metadata-8.6.1 mlflow-2.22.0 mlflow-skinny-2.22.0 opentelemetry-api-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 pyarrow-19.0.1 pydantic-2.11.5 pydantic-core-2.33.2 requests-2.32.3 smmap-5.0.2 sqlparse-0.5.3 starlette-0.46.2 typing-extensions-4.13.2 typing-inspection-0.4.1 uvicorn-0.34.2 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "### Q1 ###\n",
    "\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90ffb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc612d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2 ###\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import click\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def dump_pickle(obj, filename: str):\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)\n",
    "\n",
    "\n",
    "def read_dataframe(filename: str):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "    if fit_dv:\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    return X, dv\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    help=\"Location where the resulting files will be saved\"\n",
    ")\n",
    "def run_data_prep(raw_data_path: str, dest_path: str, dataset: str = \"green\"):\n",
    "    # Load parquet files\n",
    "    df_train = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-01.parquet\")\n",
    "    )\n",
    "    df_val = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-02.parquet\")\n",
    "    )\n",
    "    df_test = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2023-03.parquet\")\n",
    "    )\n",
    "\n",
    "    # Extract the target\n",
    "    target = 'duration'\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "    y_test = df_test[target].values\n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    dv = DictVectorizer()\n",
    "    X_train, dv = preprocess(df_train, dv, fit_dv=True)\n",
    "    X_val, _ = preprocess(df_val, dv, fit_dv=False)\n",
    "    X_test, _ = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "    # Save DictVectorizer and datasets\n",
    "    dump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n",
    "    dump_pickle((X_train, y_train), os.path.join(dest_path, \"train.pkl\"))\n",
    "    dump_pickle((X_val, y_val), os.path.join(dest_path, \"val.pkl\"))\n",
    "    dump_pickle((X_test, y_test), os.path.join(dest_path, \"test.pkl\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_data_prep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2184b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/train.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 01:23:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpnsm0zsri/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.4.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.431162180141208\n"
     ]
    }
   ],
   "source": [
    "### Q3 ###\n",
    "import os\n",
    "import pickle\n",
    "import click\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "def load_pickle(filename: str):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--data_path\",\n",
    "    default=\"./output\",\n",
    "    help=\"Location where the processed NYC taxi trip data was saved\"\n",
    ")\n",
    "def run_train(data_path: str):\n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "    mlflow.set_experiment(\"/my-mlops-experiment-1\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "        X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "        rf = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_val)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        # Optional: log the model artifact\n",
    "        mlflow.sklearn.log_model(rf, artifact_path=\"random_forest_model\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4 ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5 ###\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import click\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "mlflow.set_experiment(\"random-forest-hyperopt\")\n",
    "\n",
    "\n",
    "def load_pickle(filename: str):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--data_path\",\n",
    "    default=\"./output\",\n",
    "    help=\"Location where the processed NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--num_trials\",\n",
    "    default=15,\n",
    "    help=\"The number of parameter evaluations for the optimizer to explore\"\n",
    ")\n",
    "def run_optimization(data_path: str, num_trials: int):\n",
    "\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "    def objective(params):\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            rf = RandomForestRegressor(**params)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_val)\n",
    "            rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 10, 50, 1)),\n",
    "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
    "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 4, 1)),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    rstate = np.random.default_rng(42)  # for reproducible results\n",
    "    fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_trials,\n",
    "        trials=Trials(),\n",
    "        rstate=rstate\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_optimization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77241cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q6 ###\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import click\n",
    "import mlflow\n",
    "\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "RF_PARAMS = ['max_depth', 'n_estimators', 'min_samples_split', 'min_samples_leaf', 'random_state']\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def train_and_log_model(data_path, params):\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "    X_test, y_test = load_pickle(os.path.join(data_path, \"test.pkl\"))\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        new_params = {}\n",
    "\n",
    "        for param in RF_PARAMS:\n",
    "            if param in params:\n",
    "                new_params[param] = int(params[param]) if param != 'random_state' else int(params[param])\n",
    "            else:\n",
    "                print(f\"Warning: Parameter '{param}' missing. Using sklearn default.\")\n",
    "                # Alternatively, you can set custom default values like:\n",
    "            \n",
    "\n",
    "        rf = RandomForestRegressor(**new_params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        mlflow.sklearn.log_model(rf, artifact_path=\"random_forest_model\")\n",
    "\n",
    "        # Evaluate model on the validation and test sets\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, rf.predict(X_val)))\n",
    "        mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--data_path\",\n",
    "    default=\"./output\",\n",
    "    help=\"Location where the processed NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--top_n\",\n",
    "    default=5,\n",
    "    type=int,\n",
    "    help=\"Number of top models that need to be evaluated to decide which one to promote\"\n",
    ")\n",
    "def run_register_model(data_path: str, top_n: int):\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Retrieve the top_n model runs and log the models\n",
    "    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=top_n,\n",
    "        order_by=[\"metrics.rmse ASC\"]\n",
    "    )\n",
    "    for run in runs:\n",
    "        train_and_log_model(data_path=data_path, params=run.data.params)\n",
    "\n",
    "    # Select the model with the lowest test RMSE\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    # best_run = client.search_runs( ...  )[0]\n",
    "    best_run = client.search_runs(\n",
    "                    experiment_ids=[experiment.experiment_id],\n",
    "                    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "                    max_results=1,\n",
    "                    order_by=[\"metrics.rmse ASC\"]\n",
    "                )[0]\n",
    "\n",
    "    # Register the best model\n",
    "    # mlflow.register_model( ... )\n",
    "    run_id = best_run.info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/random_forest_model\"  # or your artifact path\n",
    "    model_name = \"nyc-taxi-duration-model\"  # Choose a meaningful name\n",
    "\n",
    "    # Register the model\n",
    "    result = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "    print(f\"Model registered as: {result.name}, version: {result.version}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_register_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
