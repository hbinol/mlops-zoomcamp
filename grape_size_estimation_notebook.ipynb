{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4881e9b0",
   "metadata": {},
   "source": [
    "# üçá Grape Size Estimation Using Ruler and SAM\n",
    "\n",
    "This notebook detects a ruler in the image using SAM, estimates the pixel-to-cm scale,\n",
    "and uses a provided grape mask to measure grape sizes in cm¬≤.\n",
    "\n",
    "Supports both **horizontal and vertical** rulers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f041a",
   "metadata": {},
   "source": [
    "How to use the following cell (config) in AWS?\n",
    "\n",
    "\t1.\tGo to Amazon SageMaker Console > Notebook instances > Lifecycle Configurations\n",
    "\t2.\tClick Create Configuration\n",
    "\t3.\tPaste the script above into the Start notebook section\n",
    "\t4.\tName it something like grape-sam-setup\n",
    "\t5.\tWhen creating your notebook instance, attach this lifecycle configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"------ Starting Lifecycle Config for Grape Phenotyping ------\"\n",
    "\n",
    "# Activate default conda environment\n",
    "source /home/ec2-user/anaconda3/bin/activate\n",
    "\n",
    "# Install system-level dependencies\n",
    "sudo yum -y update\n",
    "sudo yum -y install tesseract\n",
    "\n",
    "# Install Python packages (use pip from base environment)\n",
    "pip install --upgrade pip\n",
    "\n",
    "pip install \\\n",
    "    opencv-python-headless \\\n",
    "    numpy \\\n",
    "    matplotlib \\\n",
    "    pytesseract \\\n",
    "    tqdm \\\n",
    "    torch \\\n",
    "    torchvision \\\n",
    "    notebook\n",
    "\n",
    "# Clone and install Segment Anything\n",
    "cd /home/ec2-user/SageMaker\n",
    "if [ ! -d \"segment-anything\" ]; then\n",
    "    git clone https://github.com/facebookresearch/segment-anything.git\n",
    "fi\n",
    "cd segment-anything\n",
    "pip install -e .\n",
    "\n",
    "# Download SAM model checkpoint if not exists\n",
    "cd /home/ec2-user/SageMaker\n",
    "if [ ! -f \"sam_vit_h_4b8939.pth\" ]; then\n",
    "    wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "fi\n",
    "\n",
    "echo \"------ Grape Phenotyping Environment Ready ------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae048e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "def load_sam():\n",
    "    sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "    return SamPredictor(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìê Preprocessing for OCR\n",
    "def preprocess_for_ocr(image, mask=None):\n",
    "    if mask is not None:\n",
    "        image = cv2.bitwise_and(image, image, mask=mask.astype(np.uint8))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, h=30)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(denoised)\n",
    "    _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d14766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Tick fallback (horizontal or vertical)\n",
    "def fallback_tick_spacing(gray):\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 30, minLineLength=10, maxLineGap=5)\n",
    "    if lines is None: raise RuntimeError(\"No ticks detected\")\n",
    "    orientations = []\n",
    "    for x1, y1, x2, y2 in lines[:,0]:\n",
    "        if abs(x1 - x2) < 5: orientations.append((min(y1, y2)))  # vertical ticks\n",
    "        elif abs(y1 - y2) < 5: orientations.append((min(x1, x2)))  # horizontal ticks\n",
    "    orientations = sorted(set(orientations))\n",
    "    spacings = [orientations[i+1] - orientations[i] for i in range(len(orientations)-1)]\n",
    "    return np.median(spacings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Compute pixel-per-cm scale\n",
    "def compute_scale(gray, image, debug=False):\n",
    "    data = pytesseract.image_to_data(gray, config='--psm 6 digits', output_type=pytesseract.Output.DICT)\n",
    "    positions = {}\n",
    "    for i, txt in enumerate(data['text']):\n",
    "        if txt.isdigit():\n",
    "            val = int(txt)\n",
    "            if 0 <= val <= 30:\n",
    "                cx = data['left'][i] + data['width'][i]//2\n",
    "                cy = data['top'][i] + data['height'][i]//2\n",
    "                positions[val] = (cx, cy)\n",
    "    for a, b in [(0, 10), (0, 5), (0, 1), (1, 2), (2, 3)]:\n",
    "        if a in positions and b in positions:\n",
    "            d = np.linalg.norm(np.array(positions[a]) - np.array(positions[b]))\n",
    "            return d / abs(b - a)\n",
    "    return fallback_tick_spacing(gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Find ruler mask from SAM outputs\n",
    "def find_ruler_mask(image, masks):\n",
    "    for m in masks:\n",
    "        cnts, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in cnts:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            ar = max(w/h, h/w)\n",
    "            if ar > 5 and min(w,h) > 20:\n",
    "                return m\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa56d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì∑ Process image and measure scale\n",
    "def measure_image(image_path, grape_mask_path=None, debug=True):\n",
    "    image = cv2.imread(image_path)\n",
    "    predictor = load_sam()\n",
    "    predictor.set_image(image)\n",
    "    h, w = image.shape[:2]\n",
    "    masks, _, _ = predictor.predict(point_coords=np.array([[w//2,h//2]]), point_labels=np.array([1]), multimask_output=True)\n",
    "    ruler_mask = find_ruler_mask(image, masks)\n",
    "    gray = preprocess_for_ocr(image, ruler_mask)\n",
    "    pixel_per_cm = compute_scale(gray, image, debug=debug)\n",
    "    print(f\"Estimated scale: {pixel_per_cm:.2f} pixels per cm\")\n",
    "    if grape_mask_path:\n",
    "        gm = cv2.imread(grape_mask_path, 0)\n",
    "        gm = (gm > 127).astype(np.uint8)\n",
    "        n, lbls, stats, _ = cv2.connectedComponentsWithStats(gm, 8)\n",
    "        sizes = [stats[i,cv2.CC_STAT_AREA]/(pixel_per_cm**2) for i in range(1, n)]\n",
    "        print(f\"Detected {len(sizes)} grapes. Example sizes: {sizes[:5]}\")\n",
    "        return sizes\n",
    "    return pixel_per_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_image(\"example_grape.jpg\", \"example_grape_mask.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
